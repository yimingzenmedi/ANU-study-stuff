{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression and Bayesian Global Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.path as mpath\n",
    "import boframework.gp as gp\n",
    "import boframework.kernels as kernels\n",
    "import boframework.acquisitions as acquisitions \n",
    "import boframework.bayesopt as bayesopt\n",
    "import importlib\n",
    "importlib.reload(gp)\n",
    "importlib.reload(kernels)\n",
    "importlib.reload(acquisitions)\n",
    "importlib.reload(bayesopt)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to maximise the following 1-dimensional function,\n",
    "$$\\mathrm{maximise}_x\\quad f(x)$$\n",
    "where\n",
    "$$f(x) = \\sin(x) + \\sin(2x) + \\epsilon$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = np.array([[-2.7, 6]])\n",
    "noise_level = 0.1\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "def f(X, noise_level=noise_level):\n",
    "    return np.sin(X) + np.sin(2 * X) + noise_level * np.random.randn(*X.shape)\n",
    "\n",
    "X_init = np.array([[-0.5], [2.2]])\n",
    "Y_init = f(X_init)\n",
    "\n",
    "# Bound our random variable X\n",
    "X = np.arange(bounds[:, 0], bounds[:, 1], 0.02).reshape(-1, 1)\n",
    "\n",
    "# First let's have a noise-free objective function\n",
    "Y = f(X, 0)\n",
    "\n",
    "#Plot optimisation objective with the appropriate noise levels\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(X, Y, 'r--', lw=2, label='Noise-free objective')\n",
    "plt.plot(X, f(X), 'bx', lw=1, alpha=0.6, label='Noisy samples')\n",
    "plt.plot(X_init, Y_init, 'kx', mew=3, label='Initial samples')\n",
    "plt.title(\"Expensive Black-Box Function\", fontdict = {'fontsize' : 20})\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_approximation(gpr, X, Y, X_sample, Y_sample, X_next=None, show_legend=False):\n",
    "    mu, std = gpr.predict(X, return_std=True)\n",
    "    plt.fill_between(X.ravel(), \n",
    "                     mu.ravel() + 1.96 * std, \n",
    "                     mu.ravel() - 1.96 * std, \n",
    "                     alpha=0.1) \n",
    "    plt.plot(X, Y, 'r--', lw=2, label='Noise-free objective')\n",
    "    plt.plot(X, mu, 'b-', lw=1, label='Surrogate function')\n",
    "    plt.plot(X_sample, Y_sample, 'kx', mew=3, label='Noisy samples')\n",
    "    if X_next:\n",
    "        plt.axvline(x=X_next, ls='--', c='k', lw=1)\n",
    "    if show_legend:\n",
    "        plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "def plot_acquisition(X, Y, X_next, show_legend=False):\n",
    "    plt.plot(X, Y, 'g-', lw=1, label='Acquisition function')\n",
    "    plt.axvline(x=X_next, ls='--', c='k', lw=1, label='Next sampling location')\n",
    "    if show_legend:\n",
    "        plt.legend()    \n",
    "    plt.grid()\n",
    "        \n",
    "def plot_convergence(X_sample, Y_sample, n_init=2):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    x = X_sample[n_init:].ravel()\n",
    "    y = Y_sample[n_init:].ravel()\n",
    "    r = range(1, len(x)+1)\n",
    "    \n",
    "    x_neighbor_dist = [np.abs(a-b) for a, b in zip(x, x[1:])]\n",
    "    y_max_watermark = np.maximum.accumulate(y)\n",
    "    \n",
    "    star = mpath.Path.unit_regular_star(6)\n",
    "    circle = mpath.Path.unit_circle()\n",
    "    # concatenate the circle with an internal cutout of the star\n",
    "    verts = np.concatenate([circle.vertices, star.vertices[::-1, ...]])\n",
    "    codes = np.concatenate([circle.codes, star.codes])\n",
    "    cut_star = mpath.Path(verts, codes)\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(r[1:], x_neighbor_dist, '--k', marker=cut_star, markersize=10)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.title('Distance between consecutive x\\'s')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(r, y_max_watermark, '--c', marker=cut_star, markersize=10)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Best Y')\n",
    "    plt.title('Value of best selected sample')\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_opt = bayesopt.BO(X_init, Y_init, f, noise_level, bounds, X=X, Y=Y, plt_appr=plot_approximation, plt_acq=plot_acquisition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability Improvement (PI)\n",
    "xi = 1\n",
    "X_sample, Y_sample = bayes_opt(acquisitions.probability_improvement, xi, 10, 'Bayesian Optimisation - with Probability Improvement')\n",
    "plot_convergence(X_sample, Y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Improvement (EI)\n",
    "xi = 0.8\n",
    "X_sample, Y_sample = bayes_opt(acquisitions.expected_improvement, xi, 10, 'Bayesian Optimisation - with Expected Improvement')\n",
    "plot_convergence(X_sample, Y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher Dimensions\n",
    "\n",
    "Let's assume that we want to approximate a higher dimensional function via the surrogate model, using a Gaussian Process regression.\n",
    "\n",
    "We have the function of a N-dimensional dataset $X$, \n",
    "$$\n",
    "f(X) = \\sin^2\\left(\\frac{||X||_F}{2}\\right) + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "where we take the Frobenius norm of $X$ per column (second dimension).\n",
    "\n",
    "Let's now investigate the benefits of performing hyper-parameter optimisation on the Gaussian process regression model during the fitting process by maximising the log marginal likelihood, before performing tasks such as Bayesian optimisation to find global optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp_2D(gx, gy, mu, X_train, Y_train, title, i):\n",
    "    ax = plt.gcf().add_subplot(1, 3, i, projection='3d')\n",
    "    ax.plot_surface(gx, gy, mu.reshape(gx.shape), cmap=cm.bwr, linewidth=0.5, alpha=0.3, antialiased=False)\n",
    "    ax.scatter(X_train[:,0], X_train[0:,1], Y_train, c=Y_train, cmap=cm.seismic)\n",
    "    ax.set_title(title)\n",
    "\n",
    "noise_2D = 0.1\n",
    "np.random.seed(3408944656)\n",
    "\n",
    "def f2D(X, noise_2D=noise_2D):\n",
    "    return np.sin(0.5 * np.linalg.norm(X, axis=1))**2 + noise_2D * np.random.randn(len(X))\n",
    "\n",
    "m52 = kernels.Matern(length_scale=1.0, variance=1.0, nu=2.5)\n",
    "gpr = gp.GPRegressor(kernel=m52, noise_level=noise_2D**2, n_restarts=20)\n",
    "\n",
    "\n",
    "rx, ry = np.arange(-10, 10, 0.1), np.arange(-10, 10, 0.1)\n",
    "gx, gy = np.meshgrid(rx, rx)\n",
    "X_2D = np.c_[gx.ravel(), gy.ravel()]\n",
    "\n",
    "Y_2D = f2D(X_2D, 0)\n",
    "\n",
    "X_2D_train = np.random.uniform(-8, 8, (100, 2))\n",
    "\n",
    "Y_2D_train = f2D(X_2D_train)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "mu_s = gpr.fit_and_predict(X_2D_train, Y_2D_train, X_2D)\n",
    "plot_gp_2D(gx, gy, mu_s, X_2D_train, Y_2D_train, f'Before hyper-parameter optimization: l={1.00} sigma^2_f={1.00}', 1)\n",
    "\n",
    "params, mu_s = gpr.fit_and_predict(X_2D_train, Y_2D_train, X_2D, optimise_fit=True)\n",
    "for key in sorted(params): print(f\"{key} : {params[key]}\")\n",
    "\n",
    "plot_gp_2D(gx, gy, mu_s, X_2D_train, Y_2D_train,\n",
    "           f'After hyper-parameter optimization: l={params[\"length_scale\"]:.2f} sigma^2_f={params[\"variance\"]:.2f}', 2)\n",
    "\n",
    "plot_gp_2D(gx, gy, Y_2D, X_2D_train, Y_2D_train, f'Original function without noise', 3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4dc8959796bbb5e57e1f033ef904488ac2bd2fddfe2792526c3c30a9a85f9066"
  },
  "kernelspec": {
   "display_name": "mlcv",
   "language": "python",
   "name": "mlcv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
