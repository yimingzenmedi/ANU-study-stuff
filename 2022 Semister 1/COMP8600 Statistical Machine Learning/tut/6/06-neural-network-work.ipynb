{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Statistical Machine Learning - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Setting up the environment\n",
    "\n",
    "In this lab, we will train a neural network as a classifer.\n",
    "\n",
    "### Assumed knowledge\n",
    "- Neural networks (lectures)\n",
    "- Classifiers (lab)\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "- Implementing a neural network and a simple optimizer\n",
    "- Calculating back-propogation formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.529906Z",
     "start_time": "2019-03-20T08:29:27.346073Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We will be working with a similar dataset to the one used in the Classification lab.\n",
    "This is a census-income dataset, which shows income levels for people in the 1994 US Census. We will predict whether a person has $\\leq \\$50000$ or $> \\$50000$ income per year.\n",
    "\n",
    "Unlike in the Classification lab, this data is not linearly separable. That is, the linear classification techniques you learnt about in previous weeks are not effective on this data.\n",
    "\n",
    "The data is included with this notebook as `06-dataset.tsv`. Load the data into a NumPy array called `data` using numpy genfromtxt function.\n",
    "The column names are given in the variable `columns` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.533246Z",
     "start_time": "2019-03-20T08:29:27.531409Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['income', 'age', 'education', 'private-work', 'married', 'capital-gain', 'capital-loss', 'hours-per-week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.719934Z",
     "start_time": "2019-03-20T08:29:27.534318Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt(\"06-dataset.tsv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will implement a neural network using only numpy functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks for a neural network\n",
    "\n",
    "Neural network libraries like PyTorch and TensorFlow seal different functionalities into different classes.\n",
    "\n",
    "First, implement the fully connected layer which does $\\mathbf{y} = X\\mathbf{w} + \\mathbf{b}$.\n",
    "It is also called Linear layer in PyTorch or Dense layer in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.727328Z",
     "start_time": "2019-03-20T08:29:27.721568Z"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnectedLayer():\n",
    "    \"\"\"\n",
    "    This is a class skeleton provided.\n",
    "    It should perform y = Xw + b and its correspongding gradient.\n",
    "    If you never defined any classes in python before, you probably want to read other tutorials.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        This is the init function where you have all the attributes needed defined.\n",
    "        You don't have to modify any thing in this function but you should read it carefully.\n",
    "        What each represents will be explained in the next few functions.\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = np.zeros((in_features, out_features))\n",
    "        self.bias = np.zeros((out_features, 1))\n",
    "        self.g_weight = np.zeros((in_features, out_features))\n",
    "        self.g_bias = np.zeros((out_features, 1))\n",
    "        self.input = None\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Currently, the weight and bias of this layer is initilized to zero, which is terrible.\n",
    "        You want to re-initilize the weight with standard normal distribution \n",
    "        and the bias with uniform distribution defined on range 0 to 1.\n",
    "        Or you can try different initilization methods.\n",
    "        After you finish, comment out raise NotImplementedError.\n",
    "        No return value is needed.\n",
    "        \"\"\"\n",
    "        ################\n",
    "        #YOUR CODE HERE#\n",
    "        ################\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Take the output of last layer as X and return the result.\n",
    "        Don't forget to save the input X to self.input. You will need the input for gradient calculation.\n",
    "        After you finish, comment out raise NotImplementedError.\n",
    "        If you are new to python/numpy, you probably want to figure out what is broadcasting \n",
    "        (see http://cs231n.github.io/python-numpy-tutorial/#numpy-broadcasting).\n",
    "        \"\"\"\n",
    "        ################\n",
    "        #YOUR CODE HERE#\n",
    "        ################\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        \n",
    "        out = None\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "    def backward_pass(self, g_next_layer):\n",
    "        \"\"\"\n",
    "        g_next_layer is the gradient passed from next layer (the layer after current layer in forward pass).\n",
    "        You need to calculate 3 things.\n",
    "        First, the gradient with respect to bias, self.g_bias.\n",
    "        Second, the gradient with respect to weights, self.g_weight.\n",
    "        Third, the gradient with respect to last layer (the layer formed by the current weight and bias), g_last_layer.\n",
    "        Save the gradient with respect to bias and the gradient with respect to weight.\n",
    "        Return the gradient with respect to last layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        ################\n",
    "        #YOUR CODE HERE#\n",
    "        ################\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "        self.g_weight = None\n",
    "        self.g_bias = None\n",
    "\n",
    "        g_last_layer = None\n",
    "\n",
    "        return g_last_layer\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weight and bias use the gradient that you just calculated.\n",
    "        No return is needed.\n",
    "        \"\"\"\n",
    "        \n",
    "        ################\n",
    "        #YOUR CODE HERE#\n",
    "        ################\n",
    "        \n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "\n",
    "Why is initialising weights and biases to zero terrible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement sigmoid function and sigmoid layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.745508Z",
     "start_time": "2019-03-20T08:29:27.738898Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    \"\"\"\n",
    "    Make sure that you function works with X being matrix.\n",
    "    Use functions in numpy instead of functions in math.\n",
    "    \"\"\"\n",
    "    ################\n",
    "    #YOUR CODE HERE#\n",
    "    ################\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.753060Z",
     "start_time": "2019-03-20T08:29:27.746788Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This is the init function where you have all the attributes needed defined.\n",
    "        You don't have to modify any thing in this function but you should read it carefully.\n",
    "        \"\"\"\n",
    "        self.input = None\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Apply sigmoid function to input and save the input for later.\n",
    "        \"\"\"\n",
    "        ################\n",
    "        #YOUR CODE HERE#\n",
    "        ################\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        \n",
    "        out = None\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward_pass(self, g_next_layer):\n",
    "        \"\"\"\n",
    "        Calculate the gradient with respect to the input.\n",
    "        g_next_layer is the gradient passed from next layer (the layer after current layer in forward pass).\n",
    "        Return the gradient with respect to the output of the last layer.\n",
    "        \"\"\"\n",
    "        ################\n",
    "        #YOUR CODE HERE#\n",
    "        ################\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "        g_last_layer = None\n",
    "\n",
    "        return g_last_layer\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        There is no parameter to update for this layer, but we still define this function to maintain a uniform interface.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "\n",
    "Why do we need the activation function to be non-linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement binary cross entropy loss and yes, this is the same loss that you used in logistic regression.\n",
    "Binary cross entropy loss can only deal with two classes. For more than two classes you need softmax function and cross entropy, but let's not worry about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.771373Z",
     "start_time": "2019-03-20T08:29:27.764962Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryCrossEntropyLoss():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This is the init function where you have all the attributes needed defined.\n",
    "        You don't have to modify any thing in this function but you should read it carefully.\n",
    "        \"\"\"\n",
    "        self.input_y = None\n",
    "        self.input_t = None\n",
    "        self.input_N = None\n",
    "\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        \"\"\"\n",
    "        y: batch_size * 1  0 <= y <= 1, the predictions\n",
    "        t: batch_size * 1 , the targets\n",
    "        (make sure y and t have the same shape. (N,) and (N,1) are different!)\n",
    "        \n",
    "        Save the input y, t and batch size N and calculate the loss.\n",
    "        Return the mean of the loss (a scalar).\n",
    "        \"\"\"\n",
    "        \n",
    "        ################\n",
    "        #YOUR CODE HERE#\n",
    "        ################\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def backward_pass(self, g_next_layer = 1):\n",
    "        \"\"\"\n",
    "        Nomrally, loss layer is the last layer in a neural network. Thus, we set the g_next_layer to 1.\n",
    "        Calculate the loss with respect to the input y.\n",
    "        \"\"\"\n",
    "        ################\n",
    "        #YOUR CODE HERE#\n",
    "        ################\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        \n",
    "        g_last_layer = None\n",
    "        \n",
    "        return g_last_layer\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        There is no parameter to update for this layer, but we still define this function to maintain a uniform interface.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a neural network with one hidden layer to solve this classification problem. \n",
    "\n",
    "### Question 3\n",
    "How many input units would there be? How many output units?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put them together\n",
    "\n",
    "Put the bulding block together to form a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.786924Z",
     "start_time": "2019-03-20T08:29:27.780084Z"
    }
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Since our simple nerual network acts sequentially, \n",
    "        we can put all layers in a list for convenient traversal\n",
    "        Initialize all layers that you need (two fully connected layers, two sigmoid layers).\n",
    "        Append them to the list in the correct order.\n",
    "        Don't forget to initilize the weights for fully connected layers. Choose a sensible hidden layer size.\n",
    "        \"\"\"\n",
    "        self.sequential = []\n",
    "        \n",
    "        ################\n",
    "        #YOUR CODE HERE#\n",
    "        ################\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "\n",
    "        for l in self.sequential:\n",
    "\n",
    "            X = l.forward_pass(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def backward_pass(self, grad):\n",
    "        \n",
    "        for l in reversed(self.sequential):\n",
    "\n",
    "            grad = l.backward_pass(grad)\n",
    "            \n",
    "\n",
    "    def update(self, learning_rate):\n",
    "\n",
    "        for l in self.sequential:\n",
    "\n",
    "            l.update(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.802105Z",
     "start_time": "2019-03-20T08:29:27.795497Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = Network()\n",
    "bce = BinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already set up a neural network.\n",
    "In the backward_pass function of each layer, you calculated the gradient of the input and the gradient of the weight.\n",
    "Now, get a pen and a paper and try to replace g_next_layer with the backward_pass function of the next layer, all the way back to binary cross entropy loss. Don't change any code, just write it down.\n",
    "\n",
    "Compare the results with the text book.\n",
    "I hope you can see how this layer structure naturally implements the chain rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network\n",
    "\n",
    "Split your data in half randomly, into a test set and a training set. Train the neural network on your training set. You may want to google train_test_split and accuracy_score of sklearn.\n",
    "If you get any memory error, make sure that all vectors in you calculate have shape (D, 1). (D, 1) and (D,) are different.\n",
    "Don't forget to normalize each feature to get mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Generate train and testing set \"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "data = np.genfromtxt(\"05-dataset.tsv\")\n",
    "\n",
    "y = data[:, 0]\n",
    "X = data[:, 1:]\n",
    "\n",
    "X_mean = X.mean(axis = 0)\n",
    "\n",
    "X -= X_mean\n",
    "X /= np.var(X, axis = 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5)\n",
    "\n",
    "training_epoch = 1000\n",
    "\n",
    "train_acc_list = np.zeros(training_epoch)\n",
    "train_loss_list = np.zeros(training_epoch)\n",
    "test_acc_list = np.zeros(training_epoch)\n",
    "test_loss_list = np.zeros(training_epoch)\n",
    "\n",
    "training_epoch = 1000\n",
    "\n",
    "train_acc_list = np.zeros(training_epoch)\n",
    "train_loss_list = np.zeros(training_epoch)\n",
    "test_acc_list = np.zeros(training_epoch)\n",
    "test_loss_list = np.zeros(training_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.807289Z",
     "start_time": "2019-03-20T08:29:27.803080Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(training_epoch):\n",
    "    \"\"\"\n",
    "    This is the main training loop.\n",
    "    You need to first run a forward pass, get the predicted probability, \n",
    "    put it into the loss function, calculate the loss and do a back pass.\n",
    "    Then update the network.\n",
    "    Calculate the test loss and the current accuracy on both the train and test sets.\n",
    "    Save these to a list for plotting later.\n",
    "    Experiment to find a good learning rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    ################\n",
    "    #YOUR CODE HERE#\n",
    "    ################\n",
    "\n",
    "    train_acc = 0    \n",
    "    train_loss = 0\n",
    "\n",
    "    grad = None\n",
    "\n",
    "    train_acc_list[i] = train_acc\n",
    "    train_loss_list[i] = train_loss\n",
    "\n",
    "    \"\"\"\n",
    "    Calcualte the accuracy and loss on the testing set.\n",
    "    \"\"\"\n",
    "    \n",
    "    ################\n",
    "    #YOUR CODE HERE#\n",
    "    ################\n",
    "\n",
    "    test_acc = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    test_acc_list[i] = test_acc\n",
    "    test_loss_list[i] = test_loss\n",
    "\n",
    "    print(\"iteration %d: train_acc %f, train_loss %f, test_acc %f, test_loss %f\" %(i+1, train_acc, train_loss, test_acc, test_loss))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots of training accuracy curve against testing accuracy curve, and training loss curve against testing loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:30:13.340163Z",
     "start_time": "2019-03-20T08:29:27.808303Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Accuracy over epochs\")\n",
    "plt.plot(train_acc_list, label=\"Train\")\n",
    "plt.plot(test_acc_list, label=\"Test\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:30:13.462006Z",
     "start_time": "2019-03-20T08:30:13.341416Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Loss over epochs\")\n",
    "plt.plot(train_loss_list, label=\"Train\")\n",
    "plt.plot(test_loss_list, label=\"Test\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an super cool website:https://playground.tensorflow.org/.\n",
    "Have fun in there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textbook Questions (optional)\n",
    "These questions are hand picked to both be of reasonable difficulty and demonstrate what you are expected to be able to solve. The questions are labelled in Bishop as either $\\star$, $\\star\\star$, or $\\star\\star\\star$ to rate its difficulty.\n",
    "\n",
    "- **Question 5.1**: First derive the relationship between $\\tanh(\\alpha)$ and $\\sigma(\\alpha)$, then use Eq. 5.4 in Bishop to compare those two neural networks. The key point is having a clear definition of the notations you will use.\n",
    "\n",
    "- **Question 5.2**: Please figure out which parts of Gaussian distribution have parameters when taking the logarithm.\n",
    "\n",
    "- **Question 5.6**: Students should always know where the activation comes from and express the derivative of logistic sigmoid function in terms of the sigmoid function itself.\n",
    "\n",
    "- **Question 5.15**: Use the symmetry between backpropagation and forward propagation.\n",
    "\n",
    "- **Question 5.25**: Challenging but important question. Show you in which case gradient descent really work. First represent the current weight with its previous value and then use mathematical induction to prove Eq. 5.197.\n",
    "\n",
    "- **Question 5.27**: Follow what Section 5.5.5 did.\n",
    "\n",
    "- **Question 5.29**: Students should understand how to take the derivative of Gaussian distribution.\n",
    "\n",
    "- **Question 5.40**: Maping binary classfication domain to multiclass domain is a very practical demand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
