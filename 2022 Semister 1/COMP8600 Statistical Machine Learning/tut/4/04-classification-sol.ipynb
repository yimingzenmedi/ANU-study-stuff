{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "###### COMP4670/8600 - Statistical Machine Learning - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will build, train, and test a logistic regression classifier.\n",
    "\n",
    "### Assumed knowledge:\n",
    "\n",
    "- Optimisation in Python (lab)\n",
    "- Regression (lab)\n",
    "- Binary classification with logistic regression (lectures)\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "\n",
    "- Implementing logistic regression\n",
    "- Practical binary classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "We will be working with the census-income dataset, which shows income levels for people in the 1994 US Census. We will predict whether a person has $\\leq \\$50000$ or $> \\$50000$ income per year.\n",
    "\n",
    "The data are included with this notebook as `04-dataset.tsv`, a textfile where in each row of data, the individual entries are delimited by tab characters. Download the data from the [course website](https://machlearn.gitlab.io/sml2020/tutorials/04-dataset.tsv)\n",
    "Load the data into a NumPy array called `data` using `numpy.genfromtxt`:\n",
    "\n",
    "```python\n",
    "    numpy.genfromtxt(filename)\n",
    "```\n",
    "\n",
    "The column names are given in the variable `columns` below.\n",
    "The `income` column are the targets, and the other columns will form our data used to try and guess the `income`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['income', 'age', 'education', 'private-work', 'married', 'capital-gain', 'capital-loss', 'hours-per-week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = np.genfromtxt(\"04-dataset.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap - Binary classification\n",
    "\n",
    "The idea behind this lab is that for each person, we want to\n",
    "try and predict if their income is above the threshold of $\\$50,000$ or not,\n",
    "based on a series of other data about their person: `age, education,...`.\n",
    "\n",
    "As per usual, for the $n^\\text{th}$ row, the first entry is the target $t_n$, and the rest\n",
    "forms the data vector $\\mathbf{x}_n$.\n",
    "\n",
    "We have two classes, $C_1$ representing the class of $ <\\$ 50,000$, which corresponds to\n",
    "a target of $t_n = 0$, and $C_2$, representing the class of $ >\\$50,000$, corresponding to\n",
    "a target of $t_n = 1$. Our objective is to learn a discriminative function $f_{\\mathbf{w}}(\\mathbf{x})$,\n",
    "parametrised by a weight vector $\\mathbf{w}$ that\n",
    "predicts which income class the person is in, based on the data given.\n",
    "\n",
    "We assume that each piece of information $(t_n, \\mathbf{x}_n)$ is i.i.d, and\n",
    "that there is some hidden probability distribution from which these target/data points are drawn.\n",
    "We will construct a likelihood function that indicates \"What is the likelihood of this particular\n",
    "weight vector $\\mathbf{w}$ having generated the observed training data $\\left\\{(t_n, \\mathbf{x}_n)\\right\\}_{n=1}^N$\".\n",
    "\n",
    "## Recap - Feature map, basis function\n",
    "\n",
    "Now some classes are not linearly seperable (we cannot draw a line such that all of one class is on one side,\n",
    "and all of the other class is on the other side). But by applying many fixed non-linear \n",
    "transformations to the inputs $\\mathbf{x}_n$ first, for some suitable choice\n",
    "of transformation $\\phi$ the result will usually be linearly separable\n",
    "(See week 3, pg 342 of the lecture slides).\n",
    "\n",
    "We let\n",
    "$$\n",
    "\\mathbf{\\phi}_n := \\phi(\\mathbf{x}_n)\n",
    "$$\n",
    "\n",
    "and work in this feature space rather than the input space.\n",
    "For the case of two classes, we could guess that the target is a linear combination of the features,\n",
    "$$\n",
    "\\hat{t}_n = \\mathbf{w}^T \\mathbf{\\phi}_n\n",
    "$$\n",
    "but $\\mathbf{w}^T \\mathbf{\\phi}_n$ is a real number, and we want $\\hat{t}_n \\in \\{0,1\\}$.\n",
    "We could threshold the result,\n",
    "$$\n",
    "\\hat{t}_n =\n",
    "\\begin{cases}\n",
    "1 & \\mathbf{w}^T \\mathbf{\\phi}_n \\geq 0 \\\\\n",
    "0 & \\mathbf{w}^T \\mathbf{\\phi}_n < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "but the discontinuity makes it impossible to define a sensible gradient. \n",
    "\n",
    "## Recap - Logistic Regression\n",
    "\n",
    "(We assume that the classes are already linearly seperable, and use our input space as our feature space.\n",
    "We also assume the data is i.i.d).\n",
    "\n",
    "Instead of using a hard threshold like above, in logistic regression\n",
    "we can use the sigmoid function $\\sigma(a)$\n",
    "$$\n",
    "\\sigma(a) := \\frac{1}{1 + e^{-a}}\n",
    "$$\n",
    "which has the intended effect of \"squishing\" the real line to the interval $[0,1]$.\n",
    "This gives a smooth version of the threshold function above, that we can differentiate.\n",
    "The numbers it returns can be interpreted as a probability of the estimated target $\\hat{t}$ belonging\n",
    "to a class $C_i$ given the element $\\phi$ of feature space. In the case of two classes, we define\n",
    "\n",
    "\\begin{align}\n",
    "p(C_1 | \\phi ) &:= \\sigma (\\mathbf{w}^T \\phi_n) = y_n \\\\\n",
    "p(C_2 | \\phi ) &:= 1 - p(C_1 | \\phi_n)= 1- y_n\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The likelihood function $p(\\mathbf{t} | \\mathbf{w}, \\mathbf{x})$ is what we want to maximise as a function\n",
    "of $\\mathbf{w}$. Since $\\mathbf{x}$ is fixed, we usually write the likelihood function as $p(\\mathbf{t} | \\mathbf{w})$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{t} | \\mathbf{w})\n",
    "&= \\prod_{n=1}^N p(t_n | \\mathbf{w}) \\\\\n",
    "&= \\prod_{n=1}^N \n",
    "\\begin{cases}\n",
    "p(C_1 | \\phi_n) & t_n = 1 \\\\\n",
    "p(C_2 | \\phi_n) & t_n = 0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "Note that\n",
    "$$\n",
    "\\begin{cases}\n",
    " y_n & t_n = 1 \\\\\n",
    "1 - y_n & t_n = 0\n",
    "\\end{cases}\n",
    "= y_n^{t_n} (1-y_n)^{1-t_n}\n",
    "$$\n",
    "as if $t_n = 1$, then $y_n^1 (1-y_n)^{1-1} = y_n$ and if $t_n = 0$ then $y_n^0 (1-y_n)^{1-0} = 1-y_n$.\n",
    "This is why we use the strange encoding of $t_n=0$ corresponds to $C_2$ and $t_n=1$ corresponds to $C_1$.\n",
    "Hence, our likelihood function is \n",
    "$$\n",
    "p(\\mathbf{t} | \\mathbf{w}) = \\prod_{n=1}^N y_n^{t_n} (1-y_n)^{1-t_n}, \\quad y_n = \\sigma(\\mathbf{w}^T \\phi_n)\n",
    "$$\n",
    "This function is quite unpleasant to try and differentiate, but we note that $p(\\mathbf{t} | \\mathbf{w})$\n",
    "is maximised when $\\log p(\\mathbf{t} | \\mathbf{w})$ is maximised.\n",
    "\\begin{align}\n",
    "\\log p(\\mathbf{t} | \\mathbf{w}) \n",
    "&= \\log \\prod_{n=1}^N y_n^{t_n} (1-y_n)^{1-t_n} \\\\\n",
    "&= \\sum_{n=1}^N \\log \\left( y_n^{t_n} (1-y_n)^{1-t_n} \\right) \\\\\n",
    "&= \\sum_{n=1}^N \\left( t_n \\log y_n +  (1-t_n) \\log (1-y_n) \\right)\n",
    "\\end{align}\n",
    "Which is maximised when $- \\log p(\\mathbf{t} | \\mathbf{w})$ is minimised, giving us our error function.\n",
    "$$\n",
    "E(\\mathbf{w}) := - \\sum_{n=1}^N \\left( t_n \\log y_n +  (1-t_n) \\log (1-y_n) \\right)\n",
    "$$\n",
    "We can then take the derivative of this, which gives us\n",
    "$$\n",
    "\\nabla_\\mathbf{w} E(\\mathbf{w}) = \\sum_{n=1}^N (y_n - t_n) \\phi_n\n",
    "$$\n",
    "\n",
    "(Note: We also usually divide the error by the number of data points, to obtain the average error. The error\n",
    "shouldn't get 10 times as large just because there is more data avaliable, so we should divide by the\n",
    "number of error points to reflect that.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Derivative of binary cross entropy\n",
    "Take the derivative of $E(\\mathbf{w})$, and show that it is equal to the above. Note that the derivative doesn't have any sigmoid functions. (Hint: Use the identity $\\sigma'(a) = \\sigma(a) \\left( 1- \\sigma(a) \\right)$ to simplify)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution:\n",
    "Given in the lecture notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. $L_2$ regularisation, Gaussian prior\n",
    "\n",
    "Now we consider an isotropic guassian prior for $\\mathbf{w}$ (i.e. $w \\sim \\mathcal{N}(\\mathbf{0},\\alpha^{-1}I) $). Use the likelihood we derived above and the Gaussian prior, show that the error function could be writen as $$ E(\\mathbf{w}) = - (\\sum_{n=1}^N \\left( t_n \\log y_n +  (1-t_n) \\log (1-y_n) \\right)) + \\frac{\\lambda}{2}\\Vert \\mathbf{w} \\Vert_2^2$$ for some $\\lambda$. Write out the relation between $\\lambda$ and $\\alpha$.\n",
    "\n",
    "Hint: Derive the negative logarithm of posterior $p(\\mathbf{w}|\\mathbf{t})$ and discard the constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution:\n",
    "\n",
    "By bayes formula, we know that $p(\\mathbf{w}|t) \\propto p(t|\\mathbf{w})p(\\mathbf{w})$. Hence, the error function could be written as\n",
    "$$\n",
    "\\begin{align}\n",
    "E(\\mathbf{w}) &= -\\log p(\\mathbf{t}|\\mathbf{w}) - \\log p(\\mathbf{w}) \\\\\n",
    "&=  - \\left(\\sum_{n=1}^N \\left( t_n \\log y_n +  (1-t_n) \\log (1-y_n) \\right)\\right) + \\frac{\\alpha}{2}\\Vert \\mathbf{w} \\Vert_2^2 + const.\n",
    "\\end{align}\n",
    "$$\n",
    "And $\\lambda = \\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Derivative of binary cross entropy with regularisation\n",
    "Take the derivative of $E(\\mathbf{w})$ again, accounting for the added regularisation term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution:\n",
    "$$\\sum_{n=1}^N (y_n - t_n) \\phi_n + \\alpha \\mathbf{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Classification with logistic regression\n",
    "\n",
    "Implement binary classification using logistic regression and $L_2$ regularisation. Make sure you write good quality code with comments and docstrings where appropriate. In this question, we use the mean error function defined above.\n",
    "\n",
    "$$ E(\\mathbf{w}) = \\frac{1}{N}\\left\\{-\\left(\\sum_{n=1}^N \\left( t_n \\log y_n +  (1-t_n) \\log (1-y_n)\\right)\\right) + \\frac{\\lambda}{2}\\Vert \\mathbf{w} \\Vert_2^2\\right\\}$$\n",
    "\n",
    "To optimise your cost function, we will implement a stochastic gradient descent algorithm by hand. We first recall that, in (full-batch) gradient descent, the iteration formula is given by $$ \\mathbf{w}_{k+1} = \\mathbf{w}_k - \\eta\\nabla_{\\mathbf{w}}E(\\mathbf{w}_k),$$ where $\\eta$ is the learning rate. For stochatsic gradient descent, instead of using the full dataset in each iteration, we will divide the dataset into several mini-batches and use the gradient with respect to one mini-batch to update the parameter in each iteration. Specificly, we first write our regulariser into the sum, \n",
    "$$ E(\\mathbf{w}) = \\frac{1}{N}\\sum_{n=1}^N \\left\\{ -\\left(t_n \\log y_n +  (1-t_n) \\log (1-y_n)\\right) + \\frac{\\lambda}{2N}\\Vert \\mathbf{w} \\Vert_2^2 \\right\\}.$$ Then, for a minibatch $\\mathcal{B}_i$ (suppose $\\mathcal{B}_i$ is a set of indices), the stochastic gradient $g_{\\mathcal{B}_i}$ could be defined as \n",
    "$$ g_{\\mathcal{B}_i}(\\mathbf{w}_k) = \\frac{1}{N_{\\mathcal{B}_i}}\\nabla_\\mathbf{w}\\left\\{\\sum_{n\\in\\mathcal{B}_i} \\left\\{ -\\left(t_n \\log y_n +  (1-t_n) \\log (1-y_n)\\right) + \\frac{\\lambda}{2N}\\Vert \\mathbf{w} \\Vert_2^2 \\right\\}\\right\\}\\Bigg\\vert_{\\mathbf{w} = \\mathbf{w}_k}.$$ In each step, the updation formula is given by $$ \\mathbf{w}_{k+1} = \\mathbf{w}_{k} - \\eta g_{\\mathcal{B}_i}(\\mathbf{w}_k).$$ Note that, for each iteration, we can choose a mini-batch in turn.\n",
    "\n",
    "\n",
    "By above equations, implement five functions:\n",
    "\n",
    "- `cost(w, X, t, a, N)`, which calculates the value of the cost function in a mini-batch,\n",
    "- `grad(w, X, t, a, N)`, which calculates the (stochastic) gradient of the cost function in a mini-batch,\n",
    "- `create_mini_batches(X_train, t_train, num_batches)`, which creates a list of mini-batch,\n",
    "- `train(X_train, t_train, a, learning_rate, num_iterations, num_batches)`, which returns the maximum likelihood weight vector using stochastic gradient desecent, and\n",
    "- `predict(w, X)`, which returns predicted class probabilities,\n",
    "\n",
    "where \n",
    "* $\\mathbf{w}$ is a weight vector, \n",
    "* $X$ is a matrix of examples, \n",
    "* $t$ is a vector of labels/targets, \n",
    "* $a$ is the regularisation weight. \n",
    "\n",
    "(We would use $\\lambda$ for the regularisation term, but `a` is easier to type than `lambda`, and\n",
    "`lambda` is a reserved keyword in python, for lambda functions).\n",
    "\n",
    "See below for expected usage.\n",
    "\n",
    "We add an extra column of ones to represent the bias term.\n",
    "\n",
    "## Note\n",
    "\n",
    "* You should use 80% of the data as your training set, and 20% of the data as your test set.\n",
    "* You also may want to normalise the data before hand. If the magnitude of $\\mathbf{w}^T \\phi_n$\n",
    "is very large, the gradient of $\\sigma(\\mathbf{w}^T \\phi_n)$ will be very near zero, which can\n",
    "cause convergence issues during numerical minimisation. If each element in a particular column is\n",
    "multiplied by a scalar (say, all elements of the `age` column) then the result is essentially the same\n",
    "as stretching the space in which the data lives. The model will also be proportionally stretched,\n",
    "but will not fundamentally change the behaviour. So by normalising each column, we can avoid\n",
    "issues related to numerical convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert data_raw.shape[1] == len(columns)\n",
    "data = np.concatenate([data_raw, np.ones((data_raw.shape[0], 1))], axis=1)  # add a column of ones\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4a. Define the loss and gradient\n",
    "Implement sigmoid function, binary cross entropy and the gradient of the error function as defined above. Note that the `cost` and `grad` function could work both in the full batch or mini-batches (by tunning the parameter `N`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-a))\n",
    "\n",
    "def cost(w, X, t, a, N): # N is the total sample size\n",
    "    y = predict(w, X)\n",
    "    l2 = (w**2).sum() * a/2\n",
    "    likelihood = t @ np.log(y) + (1 - t) @ np.log(1 - y)\n",
    "    cost = (l2 - likelihood)/N\n",
    "    return cost\n",
    "\n",
    "def stochastic_grad(w, X, t, a, N, N_batch): # N is the total sample size\n",
    "    y = predict(w, X)\n",
    "    return ((X * (y.reshape(t.shape) - t).reshape((X.shape[0],1))).sum(axis=0))/N_batch + (a/N) * w\n",
    "\n",
    "def batch_grad(w, X, t, a, N): # N is the total sample size\n",
    "    y = predict(w, X)\n",
    "    return ((X * (y - t)[:, None]).sum(axis=0) + a * w )/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for splitting data into train and test with ratio 80/20\n",
    "\n",
    "N = np.shape(data)[0]\n",
    "num_train = int(0.8*N)\n",
    "t_train = data[:num_train,0]\n",
    "t_test = data[num_train:, 0]\n",
    "#32561\n",
    "norm_data = data[:, 1:] / data[:, 1:].sum(axis=0, keepdims=True)\n",
    "X_train = norm_data[:num_train, :]\n",
    "X_test = norm_data[num_train:, :]\n",
    "assert X_test.shape[1] == len(columns)\n",
    "x_columns = columns[1:] + ['bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4b. Divide mini-batches\n",
    "Given the training set and the number of minibatches we want, implememt the function `create_mini_batches` which will return a list of tuples such that each tuple represents a mini-batch which contains features and corresponding targets. (i.e. the output should be $[(X_1,t_1), (X_2,t_2), \\ldots, (X_{num\\_batches},t_{num\\_batches})])$\n",
    "\n",
    "Hint: Use `np.random.shuffle()` to shuffle the dataset first. Make sure training data are spread as evenly as you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(X_train, t_train, num_batches):\n",
    "    mini_batches = []\n",
    "    data = np.hstack((X_train, t_train.reshape((X_train.shape[0],1))))\n",
    "    np.random.shuffle(data)\n",
    "    batch_size = data.shape[0] // num_batches\n",
    "  \n",
    "    for i in range(num_batches + 1):\n",
    "        mini_batch = data[i * batch_size:(i + 1)*batch_size, :]\n",
    "        X_mini = mini_batch[:, :-1]\n",
    "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
    "        mini_batches.append((X_mini, Y_mini))\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4c. Train the model\n",
    "Implement the function `train(mini_batches, a, learning_rate, num_iterations)` which returns the maximum likelihood weight vector using stochastic gradient desecent. You can tune your `learning_rate` and `num_iterations` to attain better performance. \n",
    "\n",
    "Hint: You can try to plot the loss-iteration curve to make sure your algorithm converge properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for training the model and making predictions\n",
    "\n",
    "def train(X_train, t_train, a, learning_rate, num_iterations, num_batches):  # a is regularisation parameter\n",
    "    \n",
    "    # Keep track of loss value\n",
    "    loss_his = []\n",
    "    \n",
    "    # Create mini-batches\n",
    "    mini_batches = create_mini_batches(X_train, t_train, num_batches)\n",
    "    \n",
    "    # Initialise parameter\n",
    "    w = np.random.randn(X_train.shape[1])\n",
    "    \n",
    "    # Train\n",
    "    for i in range(num_iterations):\n",
    "        batch_num = np.mod(i, num_batches)\n",
    "        s_grad = stochastic_grad(w, mini_batches[batch_num][0], mini_batches[batch_num][1],a, 32561,mini_batches[batch_num][0].shape[0])\n",
    "        w -= learning_rate * s_grad\n",
    "        loss = cost(w, X_train, t_train, a, X_train.shape[0])\n",
    "        loss_his.append(loss)\n",
    "        \n",
    "        # print the loss per 1000 iters\n",
    "        if np.mod(i,1000) == 0:\n",
    "            print(\"*\"*40)\n",
    "            print(\"The loss in iteration \",i, \" is \",loss)\n",
    "    \n",
    "    \n",
    "    plt.xlabel(\"# iterations\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(loss_his)\n",
    "    \n",
    "    return w\n",
    "\n",
    "def predict(w, X):\n",
    "    return sigmoid(X @ w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4d. Make predictions\n",
    "Use the code above to train your model by calculating the weights for your model and making predictions for the test data.\n",
    "\n",
    "Hint: Compare your training loss with the one using `opt.fmin_bfgs`, your resulting loss should be better or roughly equal to the one computed by inbuilt functions.\n",
    "This is not easy to train, please restart and potentially tune the hyperparameters if youre loss diverge or you get a bad generalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "The loss in iteration  0  is  0.6931370189314874\n",
      "****************************************\n",
      "The loss in iteration  1000  is  0.6931334911959091\n",
      "****************************************\n",
      "The loss in iteration  2000  is  0.6931302023355682\n",
      "****************************************\n",
      "The loss in iteration  3000  is  0.6931271372393168\n",
      "****************************************\n",
      "The loss in iteration  4000  is  0.6931242817235441\n",
      "****************************************\n",
      "The loss in iteration  5000  is  0.6931216224760548\n",
      "****************************************\n",
      "The loss in iteration  6000  is  0.6931191470033191\n",
      "****************************************\n",
      "The loss in iteration  7000  is  0.6931168435808911\n",
      "****************************************\n",
      "The loss in iteration  8000  is  0.6931147012068094\n",
      "****************************************\n",
      "The loss in iteration  9000  is  0.6931127095577978\n",
      "****************************************\n",
      "The loss in iteration  10000  is  0.6931108589481018\n",
      "****************************************\n",
      "The loss in iteration  11000  is  0.6931091402907986\n",
      "****************************************\n",
      "The loss in iteration  12000  is  0.6931075450614386\n",
      "****************************************\n",
      "The loss in iteration  13000  is  0.6931060652638716\n",
      "****************************************\n",
      "The loss in iteration  14000  is  0.6931046933981337\n",
      "****************************************\n",
      "The loss in iteration  15000  is  0.6931034224302652\n",
      "****************************************\n",
      "The loss in iteration  16000  is  0.6931022457639475\n",
      "****************************************\n",
      "The loss in iteration  17000  is  0.6931011572138513\n",
      "****************************************\n",
      "The loss in iteration  18000  is  0.6931001509805871\n",
      "****************************************\n",
      "The loss in iteration  19000  is  0.6930992216271696\n",
      "****************************************\n",
      "The loss in iteration  20000  is  0.6930983640569004\n",
      "****************************************\n",
      "The loss in iteration  21000  is  0.6930975734925846\n",
      "****************************************\n",
      "The loss in iteration  22000  is  0.6930968454570036\n",
      "****************************************\n",
      "The loss in iteration  23000  is  0.6930961757545641\n",
      "****************************************\n",
      "The loss in iteration  24000  is  0.6930955604540591\n",
      "****************************************\n",
      "The loss in iteration  25000  is  0.6930949958724683\n",
      "****************************************\n",
      "The loss in iteration  26000  is  0.6930944785597379\n",
      "****************************************\n",
      "The loss in iteration  27000  is  0.6930940052844822\n",
      "****************************************\n",
      "The loss in iteration  28000  is  0.6930935730205491\n",
      "****************************************\n",
      "The loss in iteration  29000  is  0.6930931789343991\n",
      "****************************************\n",
      "The loss in iteration  30000  is  0.6930928203732502\n",
      "****************************************\n",
      "The loss in iteration  31000  is  0.6930924948539374\n",
      "****************************************\n",
      "The loss in iteration  32000  is  0.6930922000524522\n",
      "****************************************\n",
      "The loss in iteration  33000  is  0.693091933794113\n",
      "****************************************\n",
      "The loss in iteration  34000  is  0.6930916940443339\n",
      "****************************************\n",
      "The loss in iteration  35000  is  0.6930914788999523\n",
      "****************************************\n",
      "The loss in iteration  36000  is  0.693091286581085\n",
      "****************************************\n",
      "The loss in iteration  37000  is  0.6930911154234802\n",
      "****************************************\n",
      "The loss in iteration  38000  is  0.6930909638713306\n",
      "****************************************\n",
      "The loss in iteration  39000  is  0.6930908304705283\n",
      "Your training loss is:\n",
      "0.6930907137879362\n",
      "The training loss using opt.fmin_bfgs is:\n",
      "0.6931167319234615\n",
      "The predictions are:\n",
      "[0.49992622 0.49993207 0.49991632 ... 0.49991889 0.49993641 0.50024475]\n",
      "The trained parameters are:\n",
      "[-1.57921928 -1.94229408 -2.00624504 -0.55232583  2.90379519 -0.02154688\n",
      " -1.63319868 -1.96151947]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_train = train(X_train, t_train, 0.1, 10, 40000, 200)  \n",
    "# TODO：This is not easy to train, please restart and potentially tune the hyperparameters if youre loss diverge or you get a bad generalisation.\n",
    "print('Your training loss is:')\n",
    "print(cost(w_train, X_train, t_train, 0.1, X_train.shape[0]))\n",
    "w_train_bfgs = opt.fmin_bfgs(\n",
    "        f=cost, fprime=batch_grad, x0=np.random.normal(scale=0.2, size=(X_train.shape[1],)), args=(X_train, t_train, 0.1, X_train.shape[0]), disp=0)\n",
    "print('The training loss using opt.fmin_bfgs is:')\n",
    "print(cost(w_train_bfgs, X_train, t_train, 0.1, X_train.shape[0]))\n",
    "t_test_pred = predict(w_train, X_test)  # TODO\n",
    "print('The predictions are:')\n",
    "print(t_test_pred)\n",
    "print('The trained parameters are:')\n",
    "print(w_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Performance measure\n",
    "\n",
    "There are many ways to compute the performance of a binary classifier. The key concept is the idea of a confusion matrix:\n",
    "\n",
    "|     &nbsp;         | &nbsp;  | Label | &nbsp;  |\n",
    "|:-------------:|:--:|:-----:|:--:|\n",
    "|     &nbsp;         |  &nbsp;  |  0    | 1  |\n",
    "|**Prediction**| 0  |    TN | FN |\n",
    "|      &nbsp;        | 1  |    FP | TP |\n",
    "\n",
    "where\n",
    "* TP - true positive\n",
    "* FP - false positive\n",
    "* FN - false negative\n",
    "* TN - true negative\n",
    "\n",
    "Implement three functions:\n",
    "\n",
    "- `confusion_matrix(y_true, y_pred)`, which returns the confusion matrix as a list of lists given a list of true labels and a list of predicted labels;\n",
    "- `accuracy(cm)`, which takes a confusion matrix and returns the accuracy; and\n",
    "- `balanced_accuracy(cm)`, which takes a confusion matrix and returns the balanced accuracy.\n",
    "\n",
    "The accuracy is defined as $\\frac{TP + TN}{n}$, where $n$ is the total number of examples. The balanced accuracy is defined as $\\frac{1}{2}\\left(\\frac{TP}{P} + \\frac{TN}{N}\\right)$, where $T$ and $N$ are the total number of positive and negative examples respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "def confusion_matrix(t_true, t_pred):\n",
    "    t_pred = t_pred > 0.5\n",
    "    t_true = t_true > 0.5\n",
    "    tp = ((t_true == t_pred) & (t_true)).sum()\n",
    "    tn = ((t_true == t_pred) & (~t_true)).sum()\n",
    "    fp = ((t_true != t_pred) & (~t_true)).sum()\n",
    "    fn = ((t_true != t_pred) & (t_true)).sum()\n",
    "    return [[tn, fn], [fp, tp]]\n",
    "\n",
    "def accuracy(cm):\n",
    "    (TN, FN), (FP, TP) = cm\n",
    "    return (TN + TP) / (TN + FN + FP + TP)\n",
    "\n",
    "def balanced_accuracy(cm):\n",
    "    (TN, FN), (FP, TP) = cm\n",
    "    return (TN / (TN + FP) + TP / (TP + FN)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy vs balanced accuracy\n",
    "\n",
    "What is the purpose of balanced accuracy? When might you prefer it to accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution\n",
    "Balanced accuracy accounts for class imbalance. In the extreme case, you would use it if you have imbalanced classes.\n",
    "For example, suppose you had some model that you were trying to learn if a patient had cancer based on\n",
    "MRI scan data, and the prior probability on any one person having cancer is 0.1%.\n",
    "Out of thousands of training examples, only a handful might actually have cancer. By just making the model always\n",
    "say \"no, this person doesn't have cancer\" it's already 99.9% accurate. During training, the model may just \n",
    "naturally do this, by overfitting to the cases where cancer was not present.\n",
    "But the penalty for a false positive \n",
    "(the model says you have cancer when you don't) is far less bad than a false negative (the model says you are okay when you do have cancer). So we need our accuracy to be weighted by the number of training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Consider the following code which computes the accuracy and balanced accuracy. Discuss the results. (Your accuaray shoud be around `[0.75,0.5]`, you can retrain the model if your accuracy is not satisfying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7933364041148472, 0.5941260431508244]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmatrix = confusion_matrix(t_test, t_test_pred)\n",
    "[accuracy(cmatrix), balanced_accuracy(cmatrix)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution\n",
    "We get higher accuracy than balanced accuracy. This means that our data are class-imbalanced, and logistic regression may be over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Looking back at the prediction task\n",
    "\n",
    "Based on your results, what feature of the dataset is most useful for determining the income level? What feature is least useful? Why?\n",
    "\n",
    "Hint: take a look at ```w_train```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('age', -1.579219284110021)\n",
      "('education', -1.9422940836578835)\n",
      "('private-work', -2.006245037207752)\n",
      "('married', -0.5523258314939528)\n",
      "('capital-gain', 2.9037951894794856)\n",
      "('capital-loss', -0.021546876511379355)\n",
      "('hours-per-week', -1.6331986818810098)\n",
      "('bias', -1.9615194689583941)\n",
      "capital-gain is the most important feature as it has the highest magnitude of weight\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# solution\n",
    "assert len(w_train) == len(x_columns)\n",
    "plt.bar(range(len(w_train)-1), w_train[:-1])\n",
    "plt.ylabel('importance')\n",
    "plt.xlabel('features')\n",
    "plt.xticks(range(len(w_train)-1), x_columns[:-1], rotation='vertical');\n",
    "print('\\n'.join(map(str, list(zip(x_columns, w_train)))))\n",
    "\n",
    "# ignore the last weight (bias):\n",
    "i_w_abs_max = np.argmax(np.abs(w_train[:-1])) \n",
    "# skip the first column, which is for the target\n",
    "print(x_columns[i_w_abs_max], 'is the most important feature as it has the highest magnitude of weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
