{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Models: Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Statistical Machine Learning - Tutorial\n",
    "\n",
    "### Assumed knowledge\n",
    "- Directed graphical models (Bayesian networks)\n",
    "- Conditional independence, joint distribution factorisation\n",
    "- D-separation and proving conditional (in)dependence based on D-separation.\n",
    "\n",
    "### After this lab, you should be comfortable with\n",
    "- Basic operations and definitions of graphical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial mainly includes three parts: Bayesian Networks (BN), Markov Random Field (MRF) and Sum Product Algorithm (Factor Graph). Before diving into the graphical models, we will first review some basic probability concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Revision\n",
    "\n",
    "Ensure that you are familiar with the following terms:\n",
    "- Joint probability distribution\n",
    "- Marginal distribution\n",
    "- Conditional distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following table defining the joint probability distribution of two variables $A \\in \\{1,2,3,4,5\\}$ and $B \\in \\{p, q, r\\}$.\n",
    "\n",
    "|  | A=$1$ | A=$2$ | A = $3$ | A = $4$ | A = $5$ |\n",
    "|--|:--:|:--:|:--:|:--:|:--:|\n",
    "|**B**=$p$|0.01|0.01|0.12|0.01|0.14|\n",
    "|**B**=$q$|0.03|0.15|0.01|0.01|0.01|\n",
    "|**B**=$r$|0.13|0.11|0.07|0.18|0.01|\n",
    "\n",
    "Below is the table in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_AB = pd.DataFrame([[0.01, 0.01, 0.12, 0.01, 0.14],\n",
    "                     [0.03, 0.15, 0.01, 0.01, 0.01],\n",
    "                     [0.13, 0.11, 0.07, 0.18, 0.01]],\n",
    "                    index=[\"p\", \"q\", \"r\"],\n",
    "                    columns=[1,2,3,4,5])\n",
    "\n",
    "print(\"Table for p(A, B)\\n\")\n",
    "print(P_AB)\n",
    "\n",
    "# Check the table is non-negative\n",
    "assert (P_AB >= 0).all().all()\n",
    "# Check if the table sums to 1\n",
    "assert np.allclose(P_AB.sum().sum(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** Compute the following probabilities:\n",
    "- $p(B)$\n",
    "- $p(B \\mid A = 2)$\n",
    "- $p(B = p \\mid A = 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical verification of Bayes rule\n",
    "\n",
    "Given the joint distribution $p(A,B)$ there are two ways to compute the posterior $p(A \\mid B)$.\n",
    "\n",
    "The first way is by using the definition of conditional probability: $p(A \\mid B) = \\frac{p(A, B)}{p(B)}$.\n",
    "\n",
    "The second way is by using the Bayes rule: $p(A \\mid B) = \\frac{p(B \\mid A)p(A)}{\\sum_A p(A,B)}$.\n",
    "\n",
    "**Exercise 2.** Compute the tables for $p(A \\mid B)$ for all values of $A$ and $B$ using both ways. Ensure that the results are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent random variables\n",
    "\n",
    "Consider the following 5 random variables.\n",
    "- **A**ches with states (False, True)\n",
    "- **B**ronchitis with states (none, mild, severe)\n",
    "- **C**ough with states (False, True)\n",
    "- **D**isease with states (healthy, carrier, sick, recovering)\n",
    "- **E**mergency with states (False, True)\n",
    "\n",
    "**Exercise 3.** How much memory is needed to store the joint probability distribution if:\n",
    "\n",
    "(a) All variables are dependent?\n",
    "\n",
    "(b) All variables are mutually independent?\n",
    "\n",
    "What does this tell us about the benefit of establishing independencies among a set of random variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Network\n",
    "\n",
    "Bayesian Network is directed graphical model expressing causal relationship between variables.\n",
    "\n",
    "Consider the following graphical model with variables described in Exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"https://machlearn.gitlab.io/sml2021/tutorials/graphical_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.** Answer the following questions:\n",
    "1. Write down the joint factorisation  for the above graph. \n",
    "2. How much memory is needed to store the joint probability distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Independence  and D-seperation\n",
    "\n",
    "Conditional independence is an important notion in graphical models. Consider three sets of nodes $X$, $Y$ and $Z$. $X$ is said to be conditionally independent of $Y$, given $Z$ (written as $X \\perp\\!\\!\\!\\perp Y | Z$)  if\n",
    "$$\n",
    "p(X, Y \\mid Z) = p(X \\mid Z) p(Y \\mid Z).\n",
    "$$\n",
    "\n",
    "\n",
    "So, given a joint distribution characterised by a directed graphical model, how do we prove/disprove that two (sets of) variables are independent conditioned on another (set of) variables? The lecture introduced us to two ways of doing so:\n",
    "- From the joint distribution, marginalise all irrelevant variables. We should now have $p(X, Y, Z)$. Then show that, based on the structure of the graph, we can factorise $p(X, Y \\mid Z)$ into $p(X \\mid Z) p(Y \\mid Z)$. An equivalent way is to show $p(X \\mid Y, Z) = p(X \\mid Z)$.\n",
    "- (D-separation) Consider all *undirected* paths from any node in $X$ to any node in $Y$. $X$ is conditionally independent of $Y$ given $Z$ if and only if every such path is *blocked*, that is, the path includes a node such that\n",
    " - The node is in set $Z$ and it is HT or TT, or\n",
    " - The node is HH, but neither the node nor any of its descendants is in set $Z$.\n",
    "\n",
    "**Example proof.**\n",
    "Below is an example to proof using D-separation that in the above graph $C \\perp\\!\\!\\!\\perp E | D$.\n",
    "\n",
    "*(Using the first way)*\n",
    "We will show that $p(E \\mid C, D) = p(E \\mid D)$. Exploiting the structure of the graph, we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(E \\mid C, D) = \\frac{p(E, C, D)}{p(C, D)}.\n",
    "\\end{align*}\n",
    "$$\n",
    "The numerator is\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(E, C, D) & = \\sum_{b} p(E \\mid D) p(D \\mid C, b) p(b) p(C) \\\\\n",
    "& = p(C) p(E \\mid D) \\sum_{b} p(D \\mid C, b) p(b) \\\\\n",
    "& = p(C) p(E \\mid D) \\sum_{b} p(D, b \\mid C) \\\\\n",
    "& = p(C) p(E \\mid D) p(D \\mid C).\n",
    "\\end{align*}\n",
    "$$\n",
    "The denominator is\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(C, D) & = \\sum_{b} p(D \\mid C, b) p(b) p(C) \\\\\n",
    "& = p(C) \\sum_{b} p(D \\mid C, b) p(b) \\\\\n",
    "& = p(C) \\sum_{b} p(D, b \\mid C) \\\\\n",
    "& = p(C) p(D \\mid C).\n",
    "\\end{align*}\n",
    "$$\n",
    "Therefore, $p(E \\mid C, D) = \\frac{p(C) p(E \\mid D) p(D \\mid C)}{p(C) p(D \\mid C)} = p(E \\mid D)$, which by definition implies that $E$ is independent of $C$ conditioned on $D$.\n",
    "\n",
    "*(Using D-separation)*\n",
    "There is only one (undirected) path from $C$ to $E$, which is $C \\rightarrow D \\rightarrow E$. This path contains node $D$, which is observed and is a HT node. Therefore, this path must be blocked. Since all paths between $C$ and $D$ are blocked, $C \\perp\\!\\!\\!\\perp E | D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.** Identify and prove whether the conditional independences holds for the following cases: \n",
    "\n",
    "(a) A and D, when B is observed.\n",
    "\n",
    "(b) B and C, when none of the variables are observed.\n",
    "\n",
    "(c) B and C, when E is observed.\n",
    "\n",
    "(d) A and C, when none of the variables are observed.\n",
    "\n",
    "(e) A and C, when B is observed.\n",
    "\n",
    "(f) A and E, when D is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating distributions for BN\n",
    "\n",
    "**Exercise 6.** Consider the following tables.\n",
    "\n",
    "|p(B) | B=n | B=m | B=s |\n",
    "|:-----:|:--:|:--:|:--:|\n",
    "|marginal| 0.97 | 0.01 | 0.02 |\n",
    "\n",
    "|p(C) | C=False | C=True |\n",
    "|:-----:|:--:|:--:|\n",
    "|marginal| 0.7 | 0.3 |\n",
    "\n",
    "| p(A\\|B) | B=n | B=m | B=s |\n",
    "|:-----:|:--:|:--:|:--:|\n",
    "|**A**=False |0.9|0.8|0.3|\n",
    "|**A**=True |0.1|0.2|0.7|\n",
    "\n",
    "| p(D\\|B,C) | B=n, C=F | B=m, C=F | B=s, C=F | B=n, C=T | B=m, C=T | B=s, C=T |\n",
    "|:-----:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|**D**=healthy   |0.9 |0.8 |0.1 |  0.3 |0.4 |0.01|\n",
    "|**D**=carrier   |0.08|0.17|0.01|  0.05|0.05|0.01|\n",
    "|**D**=sick      |0.01|0.01|0.87|  0.05|0.15|0.97|\n",
    "|**D**=recovering|0.01|0.02|0.02|  0.6 |0.4 |0.01|\n",
    "\n",
    "\n",
    "| p(E\\|D) | D=h | D=c | D=s | D=r |\n",
    "|:-----:|:--:|:--:|:--:|:--:|\n",
    "|**E**=False | 0.99 | 0.99| 0.4| 0.9|\n",
    "|**E**=True | 0.01| 0.01| 0.6| 0.1|\n",
    "\n",
    "Compute the following:\n",
    "- $p(A,B,C,D,E)$\n",
    "- $p(E)$\n",
    "- $p(E \\mid B=s)$\n",
    "- $p(E \\mid B=s, C=T)$\n",
    "\n",
    "Note that there are two ways of arriving at the distributions:\n",
    "1. By computing p(A,B,C,D,E) and marginalising and conditioning appropriately\n",
    "2. By only computing the required distributions directly using the graphical model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textbook Questions\n",
    "\n",
    "These exercises also include ones about Markov random fields, which we will cover next week.\n",
    "\n",
    "- Q8.20: Induction on graph structure (recall from MATH1005/6005) (Difficulty $\\star$)\n",
    "- Q8.21: Note typo: it should be $f_s(x_s)$ (Difficulty $\\star\\star$)\n",
    "- Q8.27: Construct example showing greedy method not working (Difficulty $\\star\\star$)\n",
    "- Q8.29: Induction on tree structure (recall from MATH1005/6005) (Difficulty $\\star\\star$)\n",
    "- Extra: Derive eq 8.74 to 8.85 w.r.t Fig 8.51\n",
    "\n",
    "- Q10.2: Solving simulataneous equations (Difficulty $\\star$)\n",
    "- Q10.3: Use lagrangian to enforce normalisation of q (Difficulty $\\star\\star$)\n",
    "- Q10.6: Hint, how to introduce log term for both p and q? (Difficulty $\\star\\star$)\n",
    "\n",
    "- Q2.44: Manipulation with a more complex conjugate to derive the posterior (Difficulty $\\star\\star$)\n",
    "\n",
    "- Q10.7: Rewritting to the form of the respective distributions. Mostly algebra. (Difficulty $\\star\\star$)\n",
    "- Q10.8: What will $b_n$ be approximated as? (Difficulty $\\star$)\n",
    "- Q10.9: Essentially, deriving 10.31 and 10.32 (Difficulty $\\star\\star$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
