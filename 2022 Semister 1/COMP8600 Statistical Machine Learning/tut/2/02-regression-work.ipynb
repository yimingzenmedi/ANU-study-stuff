{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Statistical Machine Learning - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will use linear regression to predict the value of a home and explore the impact of regularisation.\n",
    "\n",
    "### Assumed knowledge\n",
    "- Maximum likelihood solution to a linear regression problem, with and without regularisation (lectures)\n",
    "- Matrix calculations in numpy (lab and precourse material)\n",
    "- Theory behind regularisation (lectures)\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "- Practical linear regression problems\n",
    "- Picking an appropriate regularisation parameter for a given problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\trace}[1]{\\operatorname{tr}\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\Norm}[1]{\\lVert#1\\rVert}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\inner}[2]{\\langle #1, #2 \\rangle}$\n",
    "$\\newcommand{\\DD}{\\mathscr{D}}$\n",
    "$\\newcommand{\\grad}[1]{\\operatorname{grad}#1}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "\n",
    "We will use [a dataset](https://machlearn.gitlab.io/sml2020/tutorials/02-dataset.csv) on the price of housing in Boston (see [description](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)). \n",
    "We aim to predict the value of a home from other factors.\n",
    "In this dataset, each row is one house. The first entry is the value of the house and we will predict it from the remaining values which have been normalised to be in the range $[-1, 1]$. The column labels are\n",
    "\n",
    "```'medv', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'```\n",
    "\n",
    "In the below cell we load the dataset. To read in the data, we use ```np.loadtxt``` with the optional argument ```delimiter=','```, as our data is comma separated rather than space separated. Then we remove the column containing the binary variable ```'chas'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with column labels\n",
    "names = ['medv', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']\n",
    "loaded_data = np.loadtxt('02-dataset.csv', delimiter=',')\n",
    "\n",
    "# remove chas\n",
    "column_idxes = list(range(len(names)))\n",
    "chas_idx = names.index('chas')\n",
    "wanted_columns = list(column_idxes)\n",
    "wanted_columns.remove(chas_idx)\n",
    "data = loaded_data[:,wanted_columns]\n",
    "data_names = list(names)\n",
    "data_names.remove('chas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has 506 rows (examples) and 13 columns (1 label and 12 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_names)\n",
    "print(np.array_str(data[:5], precision=3))\n",
    "assert data.shape == (506,13), data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression without regularisation\n",
    "\n",
    "Implement a **function** to find the maximum likelihood solution $w_{ML}$ assuming Gaussian noise for this linear regression problem. Remember from the lectures that this is equivalent to a linear regresion problem with the cost function set as the sum of squares error. See (3.15) in Bishop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_ml_unregularised(Phi, t):\n",
    "    # Phi: Features (N,D), t: Targets (N,)\n",
    "    # returns w_ml_unregularised (D,)\n",
    "    ### TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "\n",
    "Use a fifth of the available data for training the model using maximum likelihood. The rest of the data is allocated to the test set. Report the root mean squared error (RMSE) for the training set and the test set.\n",
    "In this cases, use the identity map as the basis function, $\\phi(x)=x$.\n",
    "\n",
    "Note that the data may be sorted or ordered in some way we cannot predict. How will you account for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    \"\"\"Randomly split data into two groups. The first group is a fifth of the data.\"\"\"\n",
    "    \n",
    "    ### TODO: implement a strategy to extract indices for training and testing instances.\n",
    "    train_idx, test_idx = None, None\n",
    "    \n",
    "    # Assume label is in the first column\n",
    "    X_train = data[train_idx, 1:]\n",
    "    t_train = data[train_idx, 0]\n",
    "    X_test = data[test_idx, 1:]\n",
    "    t_test = data[test_idx, 0]\n",
    "    \n",
    "    return X_train, t_train, X_test, t_test\n",
    "\n",
    "def rmse(X_train, t_train, X_test, t_test, w):\n",
    "    \"\"\"Return the RMSE for training and test sets\"\"\"\n",
    "    \n",
    "    def rmse_template(X, t, w):\n",
    "        # Data: X (N,D)\n",
    "        # Targets: t (N,)\n",
    "        # Params: w (D,)\n",
    "        \n",
    "        ### TODO\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # Training error\n",
    "    rmse_train = rmse_template(X_train, t_train, w)\n",
    "    # Testing error\n",
    "    rmse_test = rmse_template(X_test, t_test, w)\n",
    "    \n",
    "    return rmse_train, rmse_test\n",
    "\n",
    "X_train, t_train, X_test, t_test = split_data(data)\n",
    "w_unreg = w_ml_unregularised(X_train, t_train)\n",
    "\n",
    "train_rmse, test_rmse = rmse(X_train, t_train, X_test, t_test, w_unreg)\n",
    "print(\"RMSE without regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the model\n",
    "\n",
    "Find the feature with the biggest magnitude of weight. The below codes provide a plot with respect of this feature, discuss what do you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "feature_names = data_names[1:]\n",
    "max_feature = None # TODO\n",
    "t_train_pred = np.dot(X_train, w_unreg)\n",
    "t_test_pred = np.dot(X_test, w_unreg)\n",
    "\n",
    "# Plotting Code\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(X_train[:,max_feature], t_train, 'b.', label='true')\n",
    "ax.plot(X_train[:,max_feature], t_train_pred, 'r.', label='predicted')\n",
    "ax.set_title('Boston house prices - training set')\n",
    "ax.set_xlabel(feature_names[max_feature])\n",
    "ax.set_ylabel('Median property value ($1000s)')\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(X_test[:,max_feature], t_test, 'b.', label='true')\n",
    "ax.plot(X_test[:,max_feature], t_test_pred, 'r.', label='predicted')\n",
    "ax.set_title('Boston house prices - test set')\n",
    "ax.set_xlabel(feature_names[max_feature])\n",
    "ax.set_ylabel('Median property value ($1000s)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with regularisation\n",
    "\n",
    "Implement a **function** to find the maximum likelihood solution $w_{reg}$ for some regularisation parameter $\\lambda > 0$ assuming Gaussian noise for this linear regression problem. See (3.28) in Bishop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_ml_regularised(Phi, t, l):\n",
    "    # Phi: Features (N,D), t: Targets (N,), l: Lambda (,)\n",
    "    # returns w_ml_unregularised (D,)    \n",
    "    ### TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calculating the RMSE on the training and test sets, evaluate the impact of regularisation for $\\lambda = 1.1$.\n",
    "\n",
    "What is the effect of regularisation? (Hint: use `np.linalg.norm` to compare the magnitude of both versions of the learned weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 1.1\n",
    "X_train, t_train, X_test, t_test = split_data(data)\n",
    "w_reg = w_ml_regularised(X_train, t_train,l)\n",
    "\n",
    "train_rmse, test_rmse = rmse(X_train, t_train, X_test, t_test, w_reg)\n",
    "print(\"RMSE with regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "### TODO: calculate the norm of w_unreg and w_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking a regularisation parameter\n",
    "\n",
    "The below code plots the RMSE on both training and test sets against the regularisation parameter $\\log(\\lambda)$ for a range of values of $\\log(\\lambda)$. Read the plot and answer the questions, also discuss with a partner on your observation:\n",
    "\n",
    "1) When is the model under-regularising and over-regularising? How do you interpret the saturation when $\\log(\\lambda)$ is extremely large? \n",
    "\n",
    "2) By looking at the second plot, can you identify what is the optimal value of $\\log(\\lambda)$?\n",
    "\n",
    "3) Which value (train and test) is the evidence for your choice in 2) and why should you choose it over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lambdas = np.linspace(-5, 10)\n",
    "# Each row is a value of lambda\n",
    "# Columns are respectively ['rmse_train_linear', 'rmse_test_linear']\n",
    "results = np.zeros((len(log_lambdas), 2))\n",
    "\n",
    "for ix, l in enumerate(log_lambdas):\n",
    "    rmse_train, rmse_test = rmse(X_train, t_train, X_test, t_test, \n",
    "                                 w_ml_regularised(X_train, t_train, pow(10,float(l))))\n",
    "    results[ix, 0] = rmse_train\n",
    "    results[ix, 1] = rmse_test\n",
    "\n",
    "#fig = plt.figure(figsize=(15,8))\n",
    "plt.title('RMSE vs Regularisation parameter $\\lambda$')\n",
    "plt.plot(log_lambdas, results[:,0], 'b-', label='train') # training data\n",
    "plt.plot(log_lambdas, results[:,1], 'r-', label='test') # test data\n",
    "plt.xlabel('log($\\lambda$)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Look closer at range(-3, 1)\n",
    "\n",
    "log_lambdas = np.linspace(-3, 1)\n",
    "# Each row is a value of lambda\n",
    "# Columns are respectively ['rmse_train_linear', 'rmse_test_linear']\n",
    "results = np.zeros((len(log_lambdas), 2))\n",
    "\n",
    "for ix, l in enumerate(log_lambdas):\n",
    "    rmse_train, rmse_test = rmse(X_train, t_train, X_test, t_test, \n",
    "                                 w_ml_regularised(X_train, t_train, pow(10,float(l))))\n",
    "    results[ix, 0] = rmse_train\n",
    "    results[ix, 1] = rmse_test\n",
    "\n",
    "plt.title('RMSE vs Regularisation parameter $\\lambda$ (zoomed)')\n",
    "plt.plot(log_lambdas, results[:,0], 'b-', label='train') # training data\n",
    "plt.plot(log_lambdas, results[:,1], 'r-', label='test') # test data\n",
    "plt.xlabel('log($\\lambda$)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis Functions\n",
    "We want to use basis functions to improve our performance. Implement subroutines for polynomial basis function of degree 2. See [the feature map based on the binomial formula](http://en.wikipedia.org/wiki/Polynomial_kernel) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_quadratic(x):    \n",
    "    \"\"\"Compute phi(x) for a single training example using quadratic basis function.\"\"\"\n",
    "    # x: Single feature vector (D,)\n",
    "    # Returns quadratic feature vector ((D+1)*(D+2)/2,)\n",
    "    \n",
    "    ### TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this to your train and test sets, and repeat the above exercise with these new features. Report what differences you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Regularisation\n",
    "print(\"### Without Regularisation ###\")\n",
    "\n",
    "Phi_train = np.array([phi_quadratic(X_train[i]) for i in range(X_train.shape[0])])\n",
    "Phi_test = np.array([phi_quadratic(X_test[i]) for i in range(X_test.shape[0])])\n",
    "\n",
    "w_unreg = w_ml_unregularised(Phi_train, t_train)\n",
    "max_feature = np.argmax(np.abs(w_unreg))\n",
    "t_train_pred = np.dot(Phi_train, w_unreg)\n",
    "t_test_pred = np.dot(Phi_test, w_unreg)\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(Phi_train[:,max_feature], t_train, 'b.', label='true')\n",
    "ax.plot(Phi_train[:,max_feature], t_train_pred, 'r.', label='predicted')\n",
    "ax.set_title('Boston house prices - training set')\n",
    "ax.set_xlabel(\"Max Feature\")\n",
    "ax.set_ylabel('Median property value ($1000s)')\n",
    "x_vals = np.linspace(-1,1)\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(Phi_test[:,max_feature], t_test, 'b.', label='true')\n",
    "ax.plot(Phi_test[:,max_feature], t_test_pred, 'r.', label='predicted')\n",
    "ax.set_title('Boston house prices - test set')\n",
    "ax.set_xlabel(\"Max Feature\")\n",
    "ax.set_ylabel('Median property value ($1000s)')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "train_rmse, test_rmse = rmse(Phi_train, t_train, Phi_test, t_test, w_unreg)\n",
    "print(\"RMSE with basis functions and without regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))\n",
    "print()\n",
    "\n",
    "# With Regularisation\n",
    "print(\"### With Regularisation ###\")\n",
    "\n",
    "w_reg = w_ml_regularised(Phi_train, t_train, 1.1)\n",
    "max_feature = np.argmax(np.abs(w_reg))\n",
    "t_train_pred = np.dot(Phi_train, w_reg)\n",
    "t_test_pred = np.dot(Phi_test, w_reg)\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(Phi_train[:,max_feature], t_train, 'b.', label='true')\n",
    "ax.plot(Phi_train[:,max_feature], t_train_pred, 'r.', label='predicted')\n",
    "ax.set_title('Boston house prices - training set')\n",
    "ax.set_xlabel(\"Max Feature\")\n",
    "ax.set_ylabel('Median property value ($1000s)')\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(Phi_test[:,max_feature], t_test, 'b.', label='true')\n",
    "ax.plot(Phi_test[:,max_feature], t_test_pred, 'r.', label='predicted')\n",
    "ax.set_title('Boston house prices - test set')\n",
    "ax.set_xlabel(\"Max Feature\")\n",
    "ax.set_ylabel('Median property value ($1000s)')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "train_rmse, test_rmse = rmse(Phi_train, t_train, Phi_test, t_test, w_reg)\n",
    "print(\"RMSE with basis functions and with regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))\n",
    "print()\n",
    "\n",
    "# RMSE vs Regularisation curve\n",
    "\n",
    "log_lambdas = np.linspace(-5, 10)\n",
    "# Each row is a value of lambda\n",
    "# Columns are respectively ['rmse_train_linear', 'rmse_test_linear']\n",
    "results = np.zeros((len(log_lambdas), 2))\n",
    "\n",
    "for ix, l in enumerate(log_lambdas):\n",
    "    rmse_train, rmse_test = rmse(Phi_train, t_train, Phi_test, t_test, \n",
    "                                 w_ml_regularised(Phi_train, t_train, pow(10,float(l))))\n",
    "    results[ix, 0] = rmse_train\n",
    "    results[ix, 1] = rmse_test\n",
    "\n",
    "plt.title('RMSE vs Regularisation parameter $\\lambda$')\n",
    "plt.plot(log_lambdas, results[:,0], 'b-', label='train') # training data\n",
    "plt.plot(log_lambdas, results[:,1], 'r-', label='test') # test data\n",
    "plt.xlabel('log($\\lambda$)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
