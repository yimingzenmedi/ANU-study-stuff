00:44:17	Harry Jordan:	assignment 3 you mean? we have 3 atm then 4 is next?
00:45:39	Yucheng Zhao:	Just to make sure that our due date is 11 Oct 12pm?
00:53:33	Aggie Zub:	no it says: Monday, 11 October 2021, 12:05 AM
00:53:54	Xuyang:	How can this prompt paradigm be used in CV tasks?
00:54:04	Aggie Zub:	which is the 5 mins extra time we get in case submission issues the due date is 11:59pm October 10, 2021
01:01:59	Xin Yang:	What does convex combination mean?
01:05:18	Qinyu Zhao:	I suppose: that‚Äôs a linear combination of points where all coefficients are non-negative and sum to 1
01:06:13	Xin Yang:	Yes I think so.. The word ‚Äúconvex‚Äù is tricky for me.
01:06:44	David Quarel:	Yep: A convex combination of (x1,...xn) is y1x1 + ... + ynxn, where yi >= 0, and y1+ ... + yn=1
01:07:04	JiajieLi:	Is there appropriate to construct mixture distribution with a continuous distribution such as gaussian and a discrete distribution such as Poisson distribution?
01:08:18	Haoyang Zhang:	so the stationary points of the mix distribution function are a bit different from the mean points of each single distribution?
01:08:36	David Quarel:	I like to think of a convex combination as all the ways you could "share" a stick of length 1 amongst N many people. Everyone gets a share that sums to 1, and everyone's share is non-negative
01:09:15	Xin Yang:	Thank you !
01:12:51	David Quarel:	@Jiajie Mixtures are usually discrete, so you have a discrete mixture of continuous models
01:15:33	Kirat Alreja:	How did we get the numbers in the mixture component?
01:15:47	Kye-Li Tan:	I assume it was randomised
01:15:53	Shuyi Chen:	Same question
01:16:02	JiajieLi:	I think it should be observed from data
01:16:29	Kye-Li Tan:	but how?
01:16:55	Qinyu Zhao:	I have the same question. Hope there is some method like Elbow Method for Kmeans.
01:17:00	iPad:	https://anu.zoom.us/j/83859790342?pwd=TWR2UnVlZUREUXVEc2tzeWg3N1NpQT09
01:17:18	iPad:	Hyperparameter
01:18:00	Kye-Li Tan:	so randomised?
01:19:13	David Quarel:	The mixing coefficients are usually learned from the data
01:19:54	Xin Yang:	Yes it is correct.
01:20:57	Yuxuan Lin:	Does the initialization of mu, sigma, pi also need to be optimized instead of randomly choosing?
01:21:06	Ke Ning:	is this xn a vector?
01:21:26	Xin Yang:	Why use log here?
01:21:36	David Quarel:	log turns the product into a sum
01:21:47	Xin Yang:	Ohhh‚Ä¶ I see!! Thank you
01:33:03	Junhao, Wang:	Never gonna give you up
01:33:13	Yuchen Liu:	NEVER GONNA LET YOU DOWN
01:33:29	Ke Ning:	233333
01:33:36	3-Rui Lu:	66666
01:33:39	Kye-Li Tan:	never gonna run around and desert you
01:36:02	Aggie Zub:	what is the definition of this <=> ?
01:36:18	Yuchen Liu:	iff
01:36:47	Aggie Zub:	so in these formulas I should read it left side iff right side?
01:37:37	Mingrui Zhao:	left implies right and right implies left at the same time
01:37:40	Mingrui Zhao:	so order doesn‚Äôt matter
01:38:11	Aggie Zub:	thank you
01:47:20	Xin Yang:	So the denominator of r nk = 1 ?
01:47:27	Ke Ning:	highest rnk -> this k-th model is the most suitable one for this xn?
01:51:37	Xuyang:	Btw, how can we know there are three distributions in GMM?
01:51:53	Yiran Wang u7079256 g4:	three peaks
01:52:03	Ning Lai:	It‚Äôs a hyper-parameter
01:52:27	JiajieLi:	I think it could be observed from the number of modes(paek) in the distribution
01:52:28	Ke Ning:	maybe you can do a clustering first 23333?
01:53:31	Haoyang Zhang:	I think kmeans just uses distance to represent the probability here.  Still do not fully get the difference between hard and soft assignment.
01:54:39	Qinyu Zhao:	If we calculate and normalize the inverse of distance in KMeans, could we also get soft assignment?
01:55:51	Ke Ning:	I think : soft: a point might be generated by this model with probability p. Hard: it has to be in this cluster with probability 1.
01:56:33	David Quarel:	Yes, that's right. I would treat soft assignments as s collection of numbers representing my confidence on which cluster this point belongs
01:57:39	Haoyang Zhang:	thanks, I understand the idea after this slide
02:00:06	Qinyu Zhao:	F
02:00:11	5 Xiangyu Chen:	F
02:00:13	Harry Jordan:	F
02:00:14	Jinghang Feng:	NO
02:00:15	G6-Youming Liu:	f
02:00:19	Ning Lai:	F
02:00:20	JiajieLi:	F
02:00:28	Yiran Wang u7079256 g4:	f
02:00:30	Shulang Liu:	F
02:00:30	Xin Yang:	f
02:00:31	Xingshu Wang:	F
02:00:31	Irene Zhang_G3:	F
02:00:32	Jeff Cheng:	t
02:00:33	Haoyang Zhang:	F
02:00:38	Linda Kwan:	f
02:00:41	Yucheng Zhao:	f
02:00:41	Yupeng Chao:	f
02:00:41	Haorong Yao:	f
02:00:51	Qinyu Zhao:	F convex combination
02:00:53	Joanna:	N
02:00:54	Kye-Li Tan:	f
02:00:57	Joanna:	F
02:01:06	Ning Lai:	It can be initialized as 1/3
02:01:39	Ning Lai:	ok
02:01:52	Qinyu Zhao:	T
02:01:52	Chunjie Liu:	f
02:01:52	Kye-Li Tan:	t
02:01:53	Yupeng Chao:	f
02:01:53	JiajieLi:	F
02:01:54	Xin Yang:	t
02:01:54	Irene Zhang_G3:	t
02:01:55	Joanna:	T
02:01:55	Ke Ning:	t
02:01:55	Siya Yan:	t
02:01:55	Mengfan Yan:	t
02:01:56	G6-Youming Liu:	t
02:01:56	Haorong Yao:	f
02:01:57	Jinghang Feng:	t
02:01:57	Xin Yang:	ttt
02:01:58	Xingshu Wang:	t
02:01:58	Ning Lai:	T
02:01:59	Yucheng Zhao:	t
02:01:59	Jiayi Shen:	T
02:02:00	Tianqi Tang:	t
02:02:00	Linda Kwan:	t
02:02:02	Shulang Liu:	F
02:02:18	Qinyu Zhao:	T
02:02:21	Xin Yang:	ttttt
02:02:22	Jinghang Feng:	t
02:02:22	Punjaya Wickramasinghe:	T
02:02:23	Haoyang Zhang:	T
02:02:23	Ke Ning:	t
02:02:23	Harry Jordan:	T
02:02:23	Ning Lai:	T
02:02:24	Yupeng Chao:	t
02:02:24	Kye-Li Tan:	t
02:02:25	Xingshu Wang:	t
02:02:25	Mengfan Yan:	t
02:02:25	Yucheng Zhao:	T
02:02:25	Shuyi Chen:	t
02:02:25	Ziye Guo:	TT
02:02:26	G6-Youming Liu:	t
02:02:26	Junhao, Wang:	T
02:02:28	Jeff Cheng:	true
02:02:29	Haorong Yao:	t
02:02:31	5 Xiangyu Chen:	t
02:02:35	Linda Kwan:	t
02:02:39	5 Xiangyu Chen:	f
02:02:40	Xin Yang:	ffffffff
02:02:42	Xingshu Wang:	f
02:02:43	G6-Youming Liu:	f\
02:02:43	Siya Yan:	f
02:02:45	Haoyang Zhang:	f
02:02:45	Yucheng Zhao:	F
02:02:46	Haorong Yao:	f
02:02:46	Ning Lai:	F
02:02:47	Mengfan Yan:	f
02:02:52	Kye-Li Tan:	f
02:02:53	Linda Kwan:	false
02:02:55	Jinghang Feng:	f
02:02:57	Irene Zhang_G3:	F
02:03:00	JiajieLi:	f
02:03:05	Joanna:	T
02:03:34	Xin Yang:	Tttttttt
02:03:35	5 Xiangyu Chen:	t
02:03:36	Haoyang Zhang:	t
02:03:36	Ning Lai:	T
02:03:37	Irene Zhang_G3:	T
02:03:38	Shuyi Chen:	t
02:03:38	Jeff Cheng:	true
02:03:38	Kye-Li Tan:	t
02:03:38	Mengfan Yan:	t
02:03:39	Xingshu Wang:	t
02:03:40	Yupeng Chao:	t
02:03:41	G6-Youming Liu:	t
02:03:41	Haorong Yao:	t
02:03:41	Yucheng Zhao:	T
02:03:42	Joanna:	T
02:03:45	Ke Ning:	t
02:03:46	Linda Kwan:	t
02:03:48	Mingyu Sheng:	t
02:03:50	wzb:	F
02:04:08	Siya Yan:	t
02:04:20	Qinyu Zhao:	About the 5th question, why didn‚Äôt we consider the prior P(theta) and P(x)? According to Bayes Rule, the likelihood is not equal to what we want.
02:04:21	Haoyang Zhang:	Is the way to calculate parameters in M step generated by their gradients?
02:05:23	Qinyu Zhao:	How to find the best number of components? I know it‚Äôs a hyperparameter ‚Ä¶. But do we have some method like Elbow Method for Kmeans?
02:06:10	Ning Lai:	Should every Gaussian has the the same ‚àë ?
02:06:43	Haoyang Zhang:	i think no
02:09:33	Qinyu Zhao:	I see! Thank you!
02:09:51	Qinyu Zhao:	I‚Äôm very confused about the meaning of mixture. Which one is correct? (1) every data point is generated by one Gaussian Distribution, and we merge all points together; (2) we merge the distribution first, and every data point is generated by the merged distribution....
02:11:06	Qinyu Zhao:	üòÇSorry for so many questions‚Ä¶
02:12:24	Ning Lai:	yes
02:13:02	Ning Lai:	thanks
02:13:30	Ke Ning:	are this lecture covered for only 1-d data set?
02:15:11	Yuchen Liu:	generate every data with each gaussian distribution then combine with each weight?
02:16:08	Ke Ning:	I think we have the data set first, and we try to find a combination of models (gaussians here) that fits these data distribution
02:16:11	Shulang Liu:	like fourier expansions?
02:16:14	G6-Youming Liu:	could we go to page 5 , how to calculate the weight of p(x1),p(x2),p(x3)?
02:16:55	Yuxuan Lin:	In A3 programming part, there's a requirement saying, ‚ÄúYou need to intelligently select mu_k just like you did with k-means‚Äù. Does it mean we will implement some intelligent algorithms like k-means++ to initialize mu_k?
02:17:22	3 - Jinwu Wang:	I still have a little doubt about sigma, if you use var to calculate sigma, then in the sigma matrix, except for (1, 1), (2, 2)... (n,n) can calculate the variance, but the other positions where the covariance needs to be solved will only be equal to 0
02:17:48	Hao Lu:	the equation p(x_n | theta) is a definition? do we have proof of this equation?
02:18:41	Qinyu Zhao:	Is GMM an extension of KMeans? I mean, if we use some probability to replace the distance in Kmeans and use soft assignment, it looks like GMM.
02:19:03	G6-Youming Liu:	got it
02:21:08	Ding Khoo:	Hi, can I share my screen to ask questions?
02:22:35	Qinyu Zhao:	If we think GMM is just fitting the data distribution, the question after class ‚ÄúIf a dataset is not generated by Gaussian distributions, it cannot be modeled
by GMM.‚Äù Is right.. Because we don‚Äôt care about the real distribution, we only use GMM to fit. The fitting may be just not precise.
02:32:33	Haosen YIN:	the sample variance means variance of the sample data points like var of [1, 1, 1] which is observed data is 0?
02:32:58	Haosen YIN:	[1, 1, 1] is what we observe
02:33:04	Haosen YIN:	and variance is 0
02:33:24	Haosen YIN:	Ok
02:33:56	Qinyu Zhao:	Thank you for your time!
02:33:59	Yuchen Liu:	Thanks
02:34:01	Ning Lai:	thanks
02:34:04	Yuxuan Lin:	Thanks Liang
02:34:11	Xin Yang:	Thank u
